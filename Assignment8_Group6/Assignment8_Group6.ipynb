{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Cleaning the Data\n",
    "\n",
    "- Remove all punctuation.\n",
    "\n",
    "- Remove all non-alphanumeric characters.\n",
    "\n",
    "- Convert all symbols to lowercase format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import log\n",
    "import re\n",
    "\n",
    "class Token:\n",
    "\n",
    "    def __init__(self, word, f0):\n",
    "        self.token = word\n",
    "        self.a_count = f0\n",
    "        self.b_count = f0\n",
    "    \n",
    "def data_cleaner(path, stop_word_path=None): # author Walter and Miguel\n",
    "    stop_words = None\n",
    "    \n",
    "    if stop_word_path:\n",
    "        print(F\"Recurring on {stop_word_path}\")\n",
    "        stop_words = data_cleaner(stop_word_path)\n",
    "    \n",
    "#     print(stop_words)\n",
    "    print(\"No Stop Words Provided\")\n",
    "    text = \"\" # define empty text variable\n",
    "    with open(path, newline='\\n') as file: # open text file and read by line\n",
    "        temp_text = file.readlines() # store read lines in temporary string\n",
    "\n",
    "    linewordlist = [] # empty list for lines, each line in this list is also a list of words\n",
    "    for lineno, linedat in enumerate(temp_text): \n",
    "        linedat = linedat.strip('''!()-[]{};:'\"\\, <>./?@#$%^&*_~''') # take out all special trailing characters\n",
    "        linedat = linedat.lower() # convert all to lowercase\n",
    "        linewordlist.extend(linedat.split())\n",
    "    \n",
    "    linewordlist = np.array(linewordlist)\n",
    "    \n",
    "    if stop_words is not None:\n",
    "        words_removed = 0\n",
    "        print(F\"Attempting to remove {len(stop_words)} Stop Words\")\n",
    "        for stop_word in stop_words:\n",
    "            linewordlist = linewordlist[linewordlist != stop_word]\n",
    "        \n",
    "    for word in linewordlist: # going through each line in the file\n",
    "        word = re.sub(r'\\W+', '', word)  # Apply regular expression to retain all alphanumeric chars.\n",
    "        text += word + \"\\n\" # append word to the cleaned text string, with a newline\n",
    "    # write text data to file\n",
    "   \n",
    "    file = open(\"cleaned.txt\", \"w\")\n",
    "    file.write(text)\n",
    "    file.close()\n",
    "    return linewordlist\n",
    "\n",
    "def wordcount_dict(filepath, fileclass, tokendict, f0=1): # author Walter, final modifications Miguel\n",
    "#     tokendict = {} # create new empty dictionary\n",
    "    with open(filepath) as file: # open file\n",
    "        for line in file: # iterate through all lines (words) in the file\n",
    "            line = line.strip('\\n') # remove the trailing newlines\n",
    "            if line not in tokendict: # check to create a new dictionary entry if token not already in dict\n",
    "                token = Token(line, f0)  # Since no token exists, create a new token as entry\n",
    "                tokendict.update({line: token}) \n",
    "                                  \n",
    "            # elif line in tokendict: # otherwise, if it does exist\n",
    "            #     count = tokendict.get(line) # take the existing count\n",
    "            #     count += 1 # iterate by 1\n",
    "            #     tokendict.update({line : count}) # update in the dictionary\n",
    "           \n",
    "            if fileclass == 'a':  # If the file belongs to class `a`\n",
    "                token = tokendict[line]\n",
    "                token.a_count += 1  # Update count for `a`, `a_count`\n",
    "                tokendict[line] = token  # Update tokendict.\n",
    "            \n",
    "            elif fileclass == 'b':  # If file belongs to class `b`\n",
    "                token = tokendict[line]\n",
    "                token.b_count += 1  # Update count for `b`, `b_count`\n",
    "                tokendict[line] = token  # Update tokendict.\n",
    "    # print(tokendict) # print out the dictionary\n",
    "    return tokendict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:  # author Miguel\n",
    "    def __init__(self):\n",
    "        self.A = 0\n",
    "        self.B = 0\n",
    "        \n",
    "    def classify(self, apath, bpath, f0, dpath, tokens=dict(), path_stops=None):  # authors Walter and Miguel\n",
    "        if f0 < 1:\n",
    "            return -228  # just chose a fun number, if f0 does not match requirements, return error\n",
    "        \n",
    "        tokens = wordcount_dict(apath, 'a', tokens, f0)  # populate dictionary with a document data\n",
    "        tokens = wordcount_dict(bpath, 'b', tokens, f0)  # populate dictionary with a document data\n",
    "        data_words = data_cleaner(dpath, stop_word_path=\"data/stop_words.txt\")  # clean the test document\n",
    "        for word in data_words:  # check all words in test document\n",
    "            if word in tokens.keys():  # compare to dictionary, increment as appropriate\n",
    "                self.A += log(tokens[word].a_count)  # increment A if A found\n",
    "                self.B += log(tokens[word].b_count)  # increment B if B found\n",
    "        \n",
    "        if self.A > self.B:\n",
    "            return F\"a, {self.A}, {self.B}\"\n",
    "        elif self.A < self.B:\n",
    "            return F\"b, {self.A}, {self.B}\"\n",
    "        elif self.A == self.B:\n",
    "            return F\"ab, {self.A}, {self.B}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: 3\n",
      "Path to text file of Stop Words: \n",
      "Path to text file of Class A Docs: \n",
      "Path to text file of Class B Docs: \n",
      "Please enter a positive non-zero number for f_0: \n",
      "Path to text file of document to classify: \n",
      "\n",
      "Running Classifier. 'a' - Republican Speaker. 'b' - Democrat Speaker\n",
      "A_PATH: data/A/Combined.txt\n",
      "B_PATH: data/B/Combined.txt\n",
      "D_PATH: data/test_speeches/obama/a223.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/A/Combined.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4430a4c24ff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nRunning Classifier. 'a' - Republican Speaker. 'b' - Democrat Speaker\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mF\"A_PATH: {path_a}\\nB_PATH: {path_b}\\nD_PATH: {path_d}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdocument_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_stops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_stops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-676da6610ef7>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self, apath, bpath, f0, dpath, tokens, path_stops)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m228\u001b[0m  \u001b[0;31m# just chose a fun number, if f0 does not match requirements, return error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordcount_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# populate dictionary with a document data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordcount_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# populate dictionary with a document data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdata_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_cleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_word_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data/stop_words.txt\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clean the test document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-39cf15a66a10>\u001b[0m in \u001b[0;36mwordcount_dict\u001b[0;34m(filepath, fileclass, tokendict, f0)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwordcount_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokendict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# author Walter, final modifications Miguel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#     tokendict = {} # create new empty dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# open file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# iterate through all lines (words) in the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# remove the trailing newlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/A/Combined.txt'"
     ]
    }
   ],
   "source": [
    "phase = input(\"Phase: \")\n",
    "if phase == '1':\n",
    "    path = input(\"Path to data file: \")\n",
    "    data_cleaner(path, stop_word_path=\"data/stop_words.txt\")\n",
    "    pass\n",
    "\n",
    "elif phase == '2':\n",
    "    path = input(\"Path to cleaned text file: \")\n",
    "    tokendict = wordcount_dict(path, 'a')\n",
    "    pass\n",
    "\n",
    "elif phase == '3':\n",
    "    classifier = Classifier()\n",
    "    path_stops = input(\"Path to text file of Stop Words: \") or \"data/stop_words.txt\"\n",
    "    path_a = input(\"Path to text file of Class A Docs: \") or \"data/A/Combined.txt\"\n",
    "    path_b = input(\"Path to text file of Class B Docs: \") or \"data/B/Combined.txt\"\n",
    "    f_0 = int(input(\"Please enter a positive non-zero number for f_0: \") or '1')\n",
    "    path_d = input(\"Path to text file of document to classify: \") or \"data/test_speeches/obama/a223.txt\"\n",
    "    dictionary = {}\n",
    "    print(\"\\nRunning Classifier. 'a' - Republican Speaker. 'b' - Democrat Speaker\")\n",
    "    print(F\"A_PATH: {path_a}\\nB_PATH: {path_b}\\nD_PATH: {path_d}\")\n",
    "    document_class = classifier.classify(path_a, path_b, f_0, path_d, dictionary, path_stops=path_stops) \n",
    "    print(document_class)\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    print(F\"Sorry, phase {phase} has not yet been implemented\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
