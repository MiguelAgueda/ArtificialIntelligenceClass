{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Cleaning the Data\n",
    "\n",
    "- Remove all punctuation.\n",
    "\n",
    "- Remove all non-alphanumeric characters.\n",
    "\n",
    "- Convert all symbols to lowercase format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import log from math:\n",
    "class Token:\n",
    "\n",
    "    def __init__(self, word, f0):\n",
    "        self.token = word\n",
    "        self.a_count = f0\n",
    "        self.b_count = f0\n",
    "    \n",
    "def data_cleaner(path: str) -> str: # author Walter and Miguel\n",
    "    \n",
    "    text = \"\" # define empty text variable\n",
    "    with open(path, newline='\\n') as file: # open text file and read by line\n",
    "        temp_text = file.readlines() # store read lines in temporary string\n",
    "\n",
    "    linewordlist = [] # empty list for lines, each line in this list is also a list of words\n",
    "    for lineno, linedat in enumerate(temp_text): \n",
    "        linedat = linedat.strip('''!()-[]{};:'\"\\, <>./?@#$%^&*_~''') # take out all special trailing characters\n",
    "        linedat = linedat.lower() # convert all to lowercase\n",
    "        linewordlist.append(linedat.split())\n",
    "    for line in linewordlist: # going through each line in the file\n",
    "        for word in line: # and each word in each line\n",
    "            word = word.strip(\"\\\"!()-[]{};:', <>./?@#$%^&*_~\") # another strip pass to ensure all characters and whitespace are gone\n",
    "            word = word.replace(\"'\", \"\")\n",
    "            text += word + \"\\n\" # append word to the cleaned text string, with a newline\n",
    "    # write text data to file\n",
    "    file = open(\"cleaned.txt\", \"w\")\n",
    "    file.write(text)\n",
    "    file.close()\n",
    "\n",
    "def wordcount_dict(filepath, fileclass, tokendict, f0): # author Walter\n",
    "    #tokendict = {} # create new empty dictionary\n",
    "    with open(filepath) as file: # open file\n",
    "        for line in file: # iterate through all lines (words) in the file\n",
    "            line = line.strip('\\n') # remove the trailing newlines\n",
    "            if line not in tokendict: # check to create a new dictionary entry if token not already in dict\n",
    "                token = Token(line, 0)  # Since no token exists, create a new token as entry\n",
    "                tokendict.update({line : token}) # update the dictionary with the token\n",
    "           \n",
    "            # elif line in tokendict: # otherwise, if it does exist\n",
    "            #     count = tokendict.get(line) # take the existing count\n",
    "            #     count += 1 # iterate by 1\n",
    "            #     tokendict.update({line : count}) # update in the dictionary\n",
    "           \n",
    "            if fileclass == 'a':  # If the file belongs to class `a`\n",
    "                token = tokendict[line]\n",
    "                token.a_count += 1  # Update count for `a`, `a_count`\n",
    "                tokendict[line] = token  # Update tokendict.\n",
    "            \n",
    "            elif fileclass == 'b':  # If file belongs to class `b`\n",
    "                token = tokendict[line]\n",
    "                token.b_count += 1  # Update count for `b`, `b_count`\n",
    "                tokendict[line] = token  # Update tokendict.\n",
    "    # print(tokendict) # print out the dictionary\n",
    "    return tokendict\n",
    "\n",
    "def classify(apath, bpath, f0, dpath, wordict):\n",
    "    if f0 < 1:\n",
    "        return -228\n",
    "    sum_a = 0\n",
    "    sum_b = 0\n",
    "    wordict = wordcount_dict(apath, a, wordict)\n",
    "    wordict = wordcount_dict(bpath, b, wordict)\n",
    "    doc_d = data_cleaner(dpath)\n",
    "    for token in doc_d:\n",
    "        if token not in wordict:\n",
    "            continue\n",
    "        sum_a += log(wordict[token].a_count)\n",
    "        sum_b += log(wordict[token].b_count)\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, phase  has not yet been implemented\n"
     ]
    }
   ],
   "source": [
    "phase = input(\"Phase: \")\n",
    "if phase == '1':\n",
    "    path = input(\"Path to data file: \")\n",
    "    text = data_cleaner(path)\n",
    "    print(text)\n",
    "if phase == '2':\n",
    "    path = input(\"Path to cleaned text file: \")\n",
    "    tokendict = wordcount_dict(path, 'a')\n",
    "if phase == '3':\n",
    "    path_a = input(\"Path to text file of Class A Docs: \")\n",
    "    path_b = input(\"Path to text file of Class B Docs: \")\n",
    "    f_0 = input(\"Please enter a positive non-ero number for f_0: \")\n",
    "    path_d = input(\"Path to text file of document to classify: \")\n",
    "    dictionary = {}\n",
    "    document_class = classify(path_a, path_b, f_0, path_d, dictionary) \n",
    "\n",
    "else:\n",
    "    print(F\"Sorry, phase {phase} has not yet been implemented\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(tokendict['trump'].a_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
